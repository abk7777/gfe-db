{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bf91641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqann.gfe import GFE\n",
    "\n",
    "dbversion = \"3420\"\n",
    "\n",
    "verbose = True\n",
    "verbosity = 1\n",
    "\n",
    "gfe_maker = GFE(verbose=verbose, \n",
    "    verbosity=verbosity,\n",
    "    load_features=False, \n",
    "    store_features=True,\n",
    "    loci=hla_loci)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef699b6",
   "metadata": {},
   "source": [
    "# gfedb_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca78758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Uncomment for Jupyter Notebook\n",
    "sys.path.append(['../','../src/'])\n",
    "\n",
    "import logging\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "import urllib.request\n",
    "from Bio import AlignIO\n",
    "from Bio.SeqFeature import SeqFeature\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from seqann.models.annotation import Annotation\n",
    "from Bio import SeqIO\n",
    "from pyard import ARD\n",
    "from csv import DictWriter\n",
    "from pathlib import Path\n",
    "from constants import *\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b9de01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output memory profile to check for leaks\n",
    "_mem_profile = True if '-p' in sys.argv else False\n",
    "\n",
    "if _mem_profile:\n",
    "    from pympler import tracker, muppy, summary\n",
    "    tr = tracker.SummaryTracker()\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "\n",
    "logging.debug(f'args: {sys.argv}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e77d17d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_hasher(seq, n=32):\n",
    "    \"\"\"Takes a nucleotide or amino acid sequence and returns a reproducible\n",
    "    integer UUID. Used to create shorter unique IDs since Neo4j cannot index \n",
    "    a full sequence. Can be also be used for any string.\"\"\"\n",
    "\n",
    "    m = hashlib.md5()\n",
    "    m.update(seq)\n",
    "\n",
    "    return str(int(m.hexdigest(), 16))[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44ee0513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hla_alignments(dbversion):\n",
    "    gen_aln = {l: {} for l in hla_loci}\n",
    "    nuc_aln = {l: {} for l in hla_loci}\n",
    "    prot_aln = {l: {} for l in hla_loci}\n",
    "\n",
    "    #logging.info(f'HLA alignments:\\n{hla_align}')\n",
    "\n",
    "    for loc in hla_align:\n",
    "        msf_gen = ''.join([data_dir, dbversion, \"/\", loc.split(\"-\")[1], \"_gen.msf\"])\n",
    "        msf_nuc = ''.join([data_dir, dbversion, \"/\", loc.split(\"-\")[1], \"_nuc.msf\"])\n",
    "        msf_prot = ''.join([data_dir, dbversion, \"/\", loc.split(\"-\")[1], \"_prot.msf\"])\n",
    "\n",
    "        logging.info(f'Loading {\"/\".join(msf_gen.split(\"/\")[-3:])}')\n",
    "        align_gen = AlignIO.read(open(msf_gen), \"msf\")\n",
    "        gen_seq = {\"HLA-\" + a.name: str(a.seq) for a in align_gen}\n",
    "        del align_gen\n",
    "        logging.info(f'{str(len(gen_seq))} genomic alignments loaded')\n",
    "        gen_aln.update({loc: gen_seq})\n",
    "\n",
    "        logging.info(f'Loading {\"/\".join(msf_nuc.split(\"/\")[-3:])}')\n",
    "        align_nuc = AlignIO.read(open(msf_nuc), \"msf\")\n",
    "        nuc_seq = {\"HLA-\" + a.name: str(a.seq) for a in align_nuc}\n",
    "        del align_nuc\n",
    "        logging.info(f'{str(len(nuc_seq))} nucleotide alignments loaded')\n",
    "        nuc_aln.update({loc: nuc_seq})\n",
    "\n",
    "        # https://github.com/ANHIG/IMGTHLA/issues/158\n",
    "        # if str(dbversion) == [\"3320\", \"3360\"]:\n",
    "        #    continue\n",
    "\n",
    "        logging.info(f'Loading {\"/\".join(msf_prot.split(\"/\")[-3:])}')\n",
    "        align_prot = AlignIO.read(open(msf_prot), \"msf\")\n",
    "        prot_seq = {\"HLA-\" + a.name: str(a.seq) for a in align_prot}\n",
    "        del align_prot\n",
    "        logging.info(f'{str(len(prot_seq))} protein alignments loaded')\n",
    "        prot_aln.update({loc: prot_seq})\n",
    "\n",
    "    return gen_aln, nuc_aln, prot_aln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18651296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(seqrecord):\n",
    "    j = 3 if len(seqrecord.features) > 3 else len(seqrecord.features)\n",
    "    fiveutr = [[\"five_prime_UTR\", SeqRecord(seq=seqrecord.features[i].extract(seqrecord.seq), id=\"1\")] for i in\n",
    "               range(0, j) if seqrecord.features[i].type != \"source\"\n",
    "               and seqrecord.features[i].type != \"CDS\" and isinstance(seqrecord.features[i], SeqFeature)\n",
    "               and not seqrecord.features[i].qualifiers]\n",
    "    feats = [[''.join([str(feat.type), \"_\", str(feat.qualifiers['number'][0])]), SeqRecord(seq=feat.extract(seqrecord.seq), id=\"1\")]\n",
    "             for feat in seqrecord.features if feat.type != \"source\"\n",
    "             and feat.type != \"CDS\" and isinstance(feat, SeqFeature)\n",
    "             and 'number' in feat.qualifiers]\n",
    "\n",
    "    threeutr = []\n",
    "    if len(seqrecord.features) > 1:\n",
    "        threeutr = [[\"three_prime_UTR\", SeqRecord(seq=seqrecord.features[i].extract(seqrecord.seq), id=\"1\")] for i in\n",
    "                    range(len(seqrecord.features) - 1, len(seqrecord.features)) if\n",
    "                    seqrecord.features[i].type != \"source\"\n",
    "                    and seqrecord.features[i].type != \"CDS\" and isinstance(seqrecord.features[i], SeqFeature)\n",
    "                    and not seqrecord.features[i].qualifiers]\n",
    "\n",
    "    feat_list = fiveutr + feats + threeutr\n",
    "    annotation = {k[0]: k[1] for k in feat_list}\n",
    "\n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "565e9980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns base pair and amino acid sequences from CDS data\n",
    "def get_cds(allele):\n",
    "\n",
    "    feat_types = [f.type for f in allele.features]\n",
    "    bp_seq = None\n",
    "    aa_seq = None\n",
    "    \n",
    "    if \"CDS\" in feat_types:\n",
    "        cds_features = allele.features[feat_types.index(\"CDS\")]\n",
    "        if 'translation' in cds_features.qualifiers:\n",
    "\n",
    "            if cds_features.location is None:\n",
    "                logging.info(f\"No CDS location for feature in allele: {allele.name}\")\n",
    "            else:\n",
    "                bp_seq = str(cds_features.extract(allele.seq))\n",
    "                aa_seq = cds_features.qualifiers['translation'][0]\n",
    "                \n",
    "    return bp_seq, aa_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b172eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streams dictionaries as rows to a file\n",
    "def append_dict_as_row(file_path, dict_row):\n",
    "\n",
    "    header = list(dict_row.keys())\n",
    "\n",
    "    # Check if file exists\n",
    "    csv_file = Path(file_path)\n",
    "    if not csv_file.is_file():\n",
    "\n",
    "        # Create the file and add the header\n",
    "        with open(file_path, 'a+', newline='') as write_obj:\n",
    "            dict_writer = DictWriter(write_obj, fieldnames=header)\n",
    "            dict_writer.writeheader()\n",
    "\n",
    "    # Do not add an else statement or the first line will be skipped\n",
    "    with open(file_path, 'a+', newline='') as write_obj:\n",
    "        dict_writer = DictWriter(write_obj, fieldnames=header)\n",
    "        dict_writer.writerow(dict_row)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edee75a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs memory of objects during execution to sheck for memory leaks\n",
    "if _mem_profile:\n",
    "    def memory_profiler(mode='all'):\n",
    "\n",
    "        # Print a summary of memory usage every n alleles\n",
    "        all_objects = muppy.get_objects()\n",
    "        sum2 = summary.summarize(all_objects)\n",
    "\n",
    "        original_stdout = sys.stdout\n",
    "\n",
    "        if mode == 'all' or mode == 'agg':\n",
    "            with open(\"summary_agg.txt\", \"a+\") as f:\n",
    "                sys.stdout = f\n",
    "                summary.print_(sum2)\n",
    "                sys.stdout = original_stdout;\n",
    "\n",
    "        if mode == 'all' or mode == 'diff':\n",
    "            with open(\"summary_diff.txt\", \"a+\") as f:\n",
    "                sys.stdout = f\n",
    "                tr.print_diff()\n",
    "                sys.stdout = original_stdout;    \n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5ee21a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Refactor build alignments\n",
    "\n",
    "def build_alignments(allele, gfe, dbversion, stream=False):\n",
    "    \"\"\"Builds genomic, nucleotide and protein alignments\"\"\"\n",
    "    \n",
    "    hla_name = allele.description.split(\",\")[0]\n",
    "    loc = allele.description.split(\",\")[0].split(\"*\")[0]\n",
    "    imgt_release = f'{dbversion[0]}.{dbversion[1:3]}.{dbversion[3]}'\n",
    "    \n",
    "    if allele.description.split(\",\")[0] in gen_aln[loc]:\n",
    "        aligned_gen = gen_aln[loc][allele.description.split(\",\")[0]]\n",
    "        gen_alignment = {\n",
    "            \"label\": \"GEN_ALIGN\",\n",
    "            \"seq_id\": seq_hasher(aligned_gen.encode('utf-8')),\n",
    "            \"gfe_name\": gfe,\n",
    "            \"hla_name\": hla_name,\n",
    "            \"a_name\": hla_name.split(\"-\")[1],\n",
    "            \"length\": len(aligned_gen),\n",
    "            \"rank\": \"0\", # TO DO: confirm how this value is derived\n",
    "            \"bp_sequence\": aligned_gen,\n",
    "            \"aa_sequence\": \"\",\n",
    "            \"imgt_release\": imgt_release # 3.24.0 instead of 3240\n",
    "        }       \n",
    "        \n",
    "    if allele.description.split(\",\")[0] in nuc_aln[loc]:\n",
    "        aligned_nuc = nuc_aln[loc][allele.description.split(\",\")[\n",
    "            0]]\n",
    "\n",
    "        nuc_alignment = {\n",
    "            \"label\": \"NUC_ALIGN\",\n",
    "            \"seq_id\": seq_hasher(aligned_nuc.encode('utf-8')),\n",
    "            \"gfe_name\": gfe,\n",
    "            \"hla_name\": hla_name,\n",
    "            \"a_name\": a_name, # hla_name.split(\"-\")[1]\n",
    "            \"length\": len(aligned_nuc),\n",
    "            \"rank\": \"0\", # TO DO: confirm how this value is derived\n",
    "            \"bp_sequence\": aligned_nuc,\n",
    "            \"aa_sequence\": \"\",\n",
    "            \"imgt_release\": imgt_release\n",
    "        }\n",
    "\n",
    "    if allele.description.split(\",\")[0] in prot_aln[loc]:\n",
    "        aligned_prot = prot_aln[loc][allele.description.split(\",\")[\n",
    "            0]]\n",
    "\n",
    "        prot_alignment = {\n",
    "            \"label\": \"PROT_ALIGN\",\n",
    "            \"seq_id\": seq_hasher(aligned_prot.encode('utf-8')),\n",
    "            \"gfe_name\": gfe,\n",
    "            \"hla_name\": hla_name,\n",
    "            \"a_name\": a_name, # hla_name.split(\"-\")[1]\n",
    "            \"length\": len(aligned_prot),\n",
    "            \"rank\": \"0\", # TO DO: confirm how this value is derived\n",
    "            \"bp_sequence\": \"\",\n",
    "            \"aa_sequence\": aligned_prot,\n",
    "            \"imgt_release\": imgt_release\n",
    "        }\n",
    "\n",
    "        \n",
    "    if stream:\n",
    "        logging.info(f'Streaming alignments to file...')\n",
    "        \n",
    "        file_path = f'{data_dir}csv/all_alignments.{dbversion}.csv'\n",
    "        \n",
    "        for alignment in [gen_alignment, nuc_alignment, prot_alignment]:\n",
    "            append_dict_as_row(file_path, alignment)\n",
    "    \n",
    "        del aligned_nuc\n",
    "        del aligned_gen\n",
    "        del aligned_prot\n",
    "        \n",
    "        return\n",
    "    else:\n",
    "        return gen_alignment, nuc_alignment, prot_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31946a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcb43b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcd2d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e427bb88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9413d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the datasets for the HLA graph\n",
    "def build_hla_graph(**kwargs):\n",
    "\n",
    "    dbversion, alignments, verbose, debug, gfe_maker, limit = \\\n",
    "        kwargs.get(\"dbversion\"), \\\n",
    "        kwargs.get(\"alignments\", False), \\\n",
    "        kwargs.get(\"verbose\", False), \\\n",
    "        kwargs.get(\"debug\", False), \\\n",
    "        kwargs.get(\"gfe_maker\"), \\\n",
    "        kwargs.get(\"limit\", None) #, \\\n",
    "        # kwargs.get(\"mem_profile\", False)\n",
    "    \n",
    "    #num_alleles = limit if limit else kwargs.get(\"num_alleles\")\n",
    "\n",
    "    def _stream_to_csv(a_gen, alignments, limit):\n",
    "\n",
    "        i = 0\n",
    "        total_time_elapsed = 0\n",
    "\n",
    "        for idx, allele in enumerate(a_gen):\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # build_gfe()\n",
    "            # build_alignments()\n",
    "            # build_features()\n",
    "\n",
    "            if hasattr(allele, 'seq'):\n",
    "                \n",
    "                # TO DO - remove if not used\n",
    "                hla_name = allele.description.split(\",\")[0]\n",
    "                loc = allele.description.split(\",\")[0].split(\"*\")[0]\n",
    "\n",
    "                if (loc in hla_loci or loc == \"DRB5\") and (len(str(allele.seq)) > 5):\n",
    "\n",
    "                    try:\n",
    "\n",
    "                        # Retrieve and stream the genomic, nucleotide and protein alignments\n",
    "                        if alignments:\n",
    "                            \n",
    "                            ###\n",
    "\n",
    "                    except Exception as err:\n",
    "                        logging.error(f'Failed to get data for allele ID {allele.id}')\n",
    "                        logging.error(err)                        \n",
    "                \n",
    "                # Build and stream the GFE rows\n",
    "                try:\n",
    "\n",
    "                    _seq = str(allele.seq)\n",
    "\n",
    "                    gfe_sequence = {\n",
    "                        \"gfe_name\": gfe,\n",
    "                        \"allele_id\": allele.id,\n",
    "                        \"locus\": loc,\n",
    "                        \"hla_name\": hla_name,\n",
    "                        \"a_name\": a_name, # hla_name.split(\"-\")[1]\n",
    "                        \"seq_id\": seq_hasher(_seq.encode('utf-8')),\n",
    "                        \"sequence\": _seq,\n",
    "                        \"length\": len(_seq),\n",
    "                        \"imgt_release\": imgt_release\n",
    "                    }\n",
    "\n",
    "                    del _seq\n",
    "\n",
    "                    logging.info(f'Streaming GFEs to file...')\n",
    "                    file_name = ''.join([data_dir, f'csv/gfe_sequences.{dbversion}.csv'])\n",
    "                    append_dict_as_row(file_name, gfe_sequence)\n",
    "\n",
    "                except Exception as err:\n",
    "                    logging.error(f'Failed to write GFE data for allele ID {allele.id}')\n",
    "                    logging.error(err)   \n",
    "\n",
    "                # Build and stream the ARD group rows\n",
    "                try:\n",
    "                    logging.info(f'Streaming groups to file...')\n",
    "                    for group in groups:\n",
    "                        group_dict = {\n",
    "                            \"gfe_name\": gfe,\n",
    "                            \"allele_id\": allele.id,\n",
    "                            \"hla_name\": hla_name,\n",
    "                            \"a_name\": a_name,\n",
    "                            \"ard_id\": group[0],\n",
    "                            \"ard_name\": group[1],\n",
    "                            \"locus\": loc,\n",
    "                            \"imgt_release\": imgt_release\n",
    "                        }\n",
    "\n",
    "                        file_path = f'{data_dir}csv/all_groups.{dbversion}.csv'\n",
    "                        append_dict_as_row(file_path, group_dict)\n",
    "\n",
    "                    del groups\n",
    "\n",
    "                except Exception as err:\n",
    "                    logging.error(f'Failed to write groups for allele {allele.id}')\n",
    "                    logging.error(err)\n",
    "\n",
    "                # Build and stream the CDS rows\n",
    "                try:\n",
    "                    # Build CDS dict for CSV export, foreign key: allele_id, hla_name\n",
    "                    bp_seq, aa_seq = get_cds(allele)\n",
    "\n",
    "                    cds = {\n",
    "                        \"gfe_name\": gfe,\n",
    "                        # \"gfe_sequence\": str(allele.seq),\n",
    "                        # \"allele_id\": allele.id,\n",
    "                        # \"hla_name\": hla_name,\n",
    "                        \"bp_seq_id\": seq_hasher(bp_seq.encode('utf-8')),\n",
    "                        \"bp_sequence\": bp_seq,\n",
    "                        \"aa_seq_id\": seq_hasher(aa_seq.encode('utf-8')),\n",
    "                        \"aa_sequence\": aa_seq,\n",
    "                        # \"imgt_release\": imgt_release\n",
    "                    }\n",
    "\n",
    "                    logging.info(f'Streaming CDS to file...')\n",
    "                    file_path = f'{data_dir}csv/all_cds.{dbversion}.csv'\n",
    "                    append_dict_as_row(file_path, cds)\n",
    "\n",
    "                    del bp_seq\n",
    "                    del aa_seq\n",
    "\n",
    "                except Exception as err:\n",
    "                    logging.error(f'Failed to write CDS data for allele {allele.id}')\n",
    "                    logging.error(err)\n",
    "\n",
    "                # Build and stream the features rows\n",
    "                def build_features()\n",
    "                try:\n",
    "                    # features preprocessing steps\n",
    "                    # 1) Convert seqann type to python dict using literal_eval\n",
    "                    # 2) add GFE foreign keys: allele_id, hla_name\n",
    "                    # 3) calculate columns: length\n",
    "\n",
    "                    # features contains list of seqann objects, converts to dict, destructive step\n",
    "                    features = \\\n",
    "                        [ast.literal_eval(str(feature) \\\n",
    "                            .replace('\\'', '\"') \\\n",
    "                            .replace('\\n', '')) \\\n",
    "                            for feature in features]               \n",
    "\n",
    "                    # Append allele id's\n",
    "                    # Note: Some alleles may have the same feature, but it may not be the same rank, \n",
    "                    # so a feature should be identified with its allele by allele_id or HLA name\n",
    "                    \n",
    "                    logging.info(f'Streaming features to file...')\n",
    "                    for feature in features:\n",
    "                        feature[\"gfe_name\"] = gfe\n",
    "                        feature[\"term\"] = feature[\"term\"].upper()\n",
    "                        feature[\"allele_id\"] = allele.id \n",
    "                        feature[\"hla_name\"] = hla_name\n",
    "                        feature[\"imgt_release\"] = imgt_release\n",
    "\n",
    "                        # Avoid null values in CSV for Neo4j import\n",
    "                        feature[\"hash_code\"] = \"none\" if not feature[\"hash_code\"] else feature[\"hash_code\"]\n",
    "\n",
    "                        file_path = f'{data_dir}csv/all_features.{dbversion}.csv'\n",
    "                        append_dict_as_row(file_path, feature)\n",
    "\n",
    "                    del features\n",
    "\n",
    "                except Exception as err:\n",
    "                    logging.error(f'Failed to write features for allele {allele.id}')\n",
    "                    logging.error(err)\n",
    "\n",
    "            elapsed_time = time.time() - start_time\n",
    "            # alleles_remaining = num_alleles - (idx + 1)\n",
    "            total_time_elapsed += elapsed_time\n",
    "            avg_time_elapsed = total_time_elapsed / (idx + 1)\n",
    "            # total_time_elapsed += ((alleles_remaining * elapsed_time) / 60)\n",
    "            # avg_time_elapsed = total_time_elapsed / num_alleles\n",
    "            # time_remaining = elapsed_time * alleles_remaining\n",
    "            \n",
    "            logging.info(f'Alleles processed: {idx + 1}')\n",
    "            # logging.info(f'Alleles remaining: {alleles_remaining}')\n",
    "            logging.info(f'Elapsed time: {round(elapsed_time, 4)}')\n",
    "            logging.info(f'Avg elapsed time: {round(avg_time_elapsed, 4)}')\n",
    "            #logging.info(f'Estimated time remaining: {time.strftime(\"%H:%M:%S\", time.gmtime(time_remaining))} minutes')\n",
    "            \n",
    "            # Break point for testing\n",
    "            if limit and idx + 1 == limit:\n",
    "                    break\n",
    "\n",
    "            # Output memory profile to check for leaks; TO DO: make a parameter for frequency of profiling\n",
    "            if _mem_profile and idx % 20 == 0:\n",
    "                memory_profiler()\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    # TO DO - remove if imgt_release is not used\n",
    "    imgt_release = f'{dbversion[0]}.{dbversion[1:3]}.{dbversion[3]}'\n",
    "    # dbversion = ''.join(dbversion.split(\".\"))\n",
    "    \n",
    "    logging.debug(f'dbversion: {dbversion}')\n",
    "    logging.debug(f'imgt_release: {imgt_release}')\n",
    "    logging.debug(f'dbversion: {dbversion}')\n",
    "\n",
    "    if alignments:\n",
    "        gen_aln, nuc_aln, prot_aln = hla_alignments(dbversion)\n",
    "        # TO DO - build alignments data\n",
    "        # build_alignments()\n",
    "\n",
    "\n",
    "\n",
    "    ###### TO DO: move DAT download to build.sh ######\n",
    "    # Downloading DAT file\n",
    "    # The github URL changed from 3350 to media\n",
    "    if int(dbversion) < 3350:\n",
    "        dat_url = ''.join([imgt_hla_raw_url, dbversion, '/hla.dat'])\n",
    "    else:\n",
    "        dat_url = ''.join([imgt_hla_media_url, dbversion, '/hla.dat'])\n",
    "\n",
    "        \n",
    "    # TO DO - delete\n",
    "    dat_file = ''.join([data_dir, 'hla.', dbversion, \".dat\"])\n",
    "\n",
    "    logging.info(\"Downloading DAT file...\")\n",
    "    if not os.path.isfile(dat_file):\n",
    "        if verbose:\n",
    "            logging.info(\"Downloading dat file from \" + dat_url)\n",
    "            \n",
    "        urllib.request.urlretrieve(dat_url, dat_file)\n",
    "\n",
    "    ###################################################    \n",
    "\n",
    "    logging.info(\"Streaming rows CSV files...\")\n",
    "    \n",
    "    # TO DO - refactor this into multiple functions for each process: \n",
    "    _stream_to_csv(\n",
    "        a_gen=a_gen, \n",
    "        alignments=alignments, \n",
    "        limit=limit)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee7ff4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dat(data_dir, dbversion):\n",
    "    \n",
    "    logging.info(\"Parsing DAT file...\")\n",
    "    dat_file = ''.join([data_dir, 'hla.', dbversion, \".dat\"])\n",
    "    \n",
    "    return SeqIO.parse(dat_file, \"imgt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef45aef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_groups(allele):\n",
    "    a_name = allele.description.split(\",\")[0].split(\"-\")[1]\n",
    "    groups = [[\"HLA-\" + ard.redux(a_name, grp), grp] if ard.redux(a_name, grp) != a_name else None for\n",
    "                grp in ard_groups]\n",
    "\n",
    "    # expre_chars = ['N', 'Q', 'L', 'S']\n",
    "    # to_second = lambda a: \":\".join(a.split(\":\")[0:2]) + \\\n",
    "    #    list(a)[-1] if list(a)[-1] in expre_chars and \\\n",
    "    #    len(a.split(\":\")) > 2 else \":\".join(a.split(\":\")[0:2])\n",
    "    # seco = [[to_second(a_name), \"2nd_FIELD\"]]\n",
    "\n",
    "    return list(filter(None, groups)) # + seco"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1658f2d3",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d28b5cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-17 17:56:07 - root - INFO - Parsing DAT file...\n",
      "2021-04-17 17:56:07 - root - INFO - Loading ARD...\n"
     ]
    }
   ],
   "source": [
    "alleles = parse_dat(data_dir, \"3420\")\n",
    "\n",
    "logging.info(\"Loading ARD...\")\n",
    "ard = ARD(dbversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a5187411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-17 17:56:12 - root - INFO - Getting GFE data for allele HLA00001.1...\n",
      "2021-04-17 17:56:14 - Logger.seqann.gfe - INFO - GFE = HLA-Aw2-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-4\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gen_aln' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-cdd5d4c2fb67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgfe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgfe_maker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gfe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mann\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0malignments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_alignments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallele\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgfe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallele\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-4889919f8346>\u001b[0m in \u001b[0;36mbuild_alignments\u001b[0;34m(allele, gfe, dbversion, stream)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mimgt_release\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{dbversion[0]}.{dbversion[1:3]}.{dbversion[3]}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mallele\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_aln\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0maligned_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_aln\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mallele\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         gen_alignment = {\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gen_aln' is not defined"
     ]
    }
   ],
   "source": [
    "for allele in alleles:\n",
    "    \n",
    "    if hasattr(allele, 'seq'):\n",
    "        \n",
    "        locus = allele.description.split(\",\")[0].split(\"*\")[0]\n",
    "        \n",
    "        if (locus in hla_loci or loc == \"DRB5\") and (len(str(allele.seq)) > 5):\n",
    "            \n",
    "            groups = get_groups(allele)\n",
    "            \n",
    "            complete_annotation = get_features(allele)\n",
    "\n",
    "            ann = Annotation(annotation=complete_annotation,\n",
    "                    method='match',\n",
    "                    complete_annotation=True)\n",
    "\n",
    "            # This process takes a long time\n",
    "            logging.info(f\"Getting GFE data for allele {allele.id}...\")\n",
    "            features, gfe = gfe_maker.get_gfe(ann, loc)\n",
    "            \n",
    "            alignments = build_alignments(allele, gfe, dbversion, stream=False)\n",
    "            \n",
    "            print(allele.seq)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd90281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
